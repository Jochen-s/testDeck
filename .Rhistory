simSD <- round(sd(mns),2)
hist(mns)
hist(runif(1000))
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(runif(40)))
hist(mns)
hist(runif(1000))
hist(mns)
hist(mns)
abline(v=simMean, col="red", lwd=2)
text(simMean+2,225,paste("Sample Mean = ",round(simMean,2)))
hist(mns)
r round(1/lambda,2)
round(1/lambda,2)
lambda <- 0.2 #as required by project
MeanDistributionExp <- function(amount,size)
{
distribution <- NULL
for (i in 1 : amount) distribution = c(distribution, mean(rexp(size,lambda)))
return(distribution)
}
mns <- meanDistExp(1000,40)
simulatedMean <- round(mean(mns),2)
simulatedSD <- round(sd(mns),2)
hist(mns)
simulatedMean
baseline = c(140,138,150,148,135)
week2 = c(132,135,151,146,130)
t.test(baseline - week2,alternative="greater") # p-value < 0.05 -> reject null -> mean greater than 0
t.test(baseline - week2,alternative="less") # p-value > 0.9567 -> can't reject null -> mean equal zero
t.test(baseline,week2,paired=T)
swirl()
load(swirl)
library(swirl)
swirl()
10/36
1-10/36
1-3/36
deck
1/52
52
4/52
0
3/52
12/52
2/51
(1.6*0.8)/2
0.64/0.8
0.64/1
mypdf
integrate(?)
info(integrate)
integrate(mypdf, lower=0, upper 1.6)
integrate(mypdf, lower=0, upper=1.6)
median()
F(x)=(x^2)/4
3
info()
1
2
0.5
4
.5
sqrt(2)
.997*.001
(1-.985)*(1-.001)
.000997/(.000997+.014985)
3.5
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
3.5
integrate(myfunc,0,2)
spop
mean(spop)
allsam
apply(allsam,1,mean)
mean(smeans)
dice_sqr
ex2_fair <- sum(dice_fair * dice_sqr)
ex2_fair-3.5^2
sum(dice_high * dice_sqr)-edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
0.94208
pbinom(2,size=5,prob=.8,lower.tail=FALSE)
qnorm(.10)
0
qnorm(.975,mean=3,sd=2)
6.92
pnorm(1200,mean=1020,sd=50,lower.tail=FALSE)
pnorm((1200-1020)/50,lower.tail=FALSE)
qnorm(.75,mean=1020,sd=50)
.53
.53
ppois(3,2.5 * 4)
pbinom(5,1000,.01)
ppois(5,1000*.01)
coinPlot(10)
coinPlot(10000)
qnorm(.95)
.6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100)
binom.test(60,100)$conf.int
mywald(.2)
ACCompar(20)
lamb <- 5/94.32
lamb +c(-1,1)*qnorm(.975)*sqrt(lamb/94.32)
poisson.test(5,94.32)$conf
myplot(2)
myplot(20)
myplot2(2)
qt(.975,2)
myplot2(20)
sleep
range(g1)
range(g2)
difference <- g2-g1
mean(difference)
s <- sd(difference)
mn + c(-1,1)*qt(.975,9)*s/sqrt(10)
t.test(difference)$conf.int
sp <- 7*15.34^2 + 20*18.23^2
ns <- 8+21-2
sp <- sqrt(sp/ns)
132.86-127.44+c(-1,1)*qt(.975,ns)*sp*sqrt(1/8+1/21)
sp <- sqrt((9*var(g1)+9*var(g2))/18)
md + c(-1,1)*qt(.975,18)*sp*sqrt(1/5)
t.test(g2,g1,paired=FALSE,var.equal=TRUE)$conf
t.test(g2,g1,paired=TRUE)$conf
num <- (15.34^2/8 + 18.23^2/21)^2
den <- 15.34^4/8^2/7 + 18.23^4/21^2/20
mydf <- num/den
132.86-127.44 +c(-1,1)*qt(.975,mydf)*sqrt(15.34^2/8 + 18.23^2/21)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
library(swirl)
swirl()
install_from_swirl("Regression Models")
exit
swirl()
Statistical Inference projec
library(ggplot2)
data(mtcars)
dim(mtcars)
mtcars[1:10]
require(graphics)
pairs(mtcars, main = "mtcars data")
coplot(mpg ~ disp | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
mtcars
t.test(mpg ~ am, data = mtcars)$p.value
round(cor(mtcars)[,1], 3)
round(cor(mtcars)[,1], 10)
round(cor(mtcars)[,1], 11)
round(cor(mtcars)[,1],)
round(cor(mtcars)[,1], 5)
round(cor(mtcars)[,1], 2
)
regression_model_mpg <- glm(mpg + hp + wt, data = mtcars)
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
regression_model_mpg <- glm(mpg + hp + wt, data = mtcars)
regression_model_mpg <- lm(mpg + hp + wt, data = mtcars)
regression_model_mpg <- lm(mpg ~ hp + wt, data = mtcars)
regression_model_mpg <- glm(mpg ~ hp + wt, data = mtcars)
summary(regression_model_mpg)$coefficients
summary(regression_model_mpg)$residuals
summary(regression_model_mpg$residuals)
regression_model_mpg <- glm(mpg ~ am + cyl + disp + hp + wt, data = mtcars)
summary(regression_model_mpg)$coefficients;summary(regression_model_mpg$residuals)
regression_model_mpg <- glm(mpg ~ am + as.factor(cyl) + disp + hp + wt, data = mtcars)
summary(regression_model_mpg)$coefficients;summary(regression_model_mpg$residuals)
step_fit <- glm(mpg ~ as.factor(am) + as.factor(cyl) + as.factor(gear) + disp + hp + drat + wt, data = mtcars)
model_fit <- stepAIC(step_fit, direction = 'both'); model_fit$anova
library(MASS)
step_fit <- glm(mpg ~ as.factor(am) + as.factor(cyl) + as.factor(gear) + disp + hp + drat + wt, data = mtcars)
model_fit <- stepAIC(step_fit, direction = 'both'); model_fit$anova
Anova_test <- glm(mpg ~ as.factor(am) + as.factor(cyl) + as.factor(gear) + disp + hp + drat + wt, data = mtcars)
Best_model <- stepAIC(Anova_test, direction = 'both'); Best_model$anova
transmission_type_relevance <- lm(mtcars$mpg ~ mtcars$am); summary(transmission_type_relevance)$coefficients
library(swirl)
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child ~ parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals,galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs-rhs
all.equal(lhs,rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
cor(gpa_nor,gch_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
fit <- lm(child ~ parent, galton)
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
u <- mean(galton$child)
mu <- mean(galton$child)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
1-sRes/sTot
summary(fit)$r.squared
or(galton$parent,galton$child)^2
cor(galton$parent,galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent - 1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
head(trees)
fit <- lm(Volume ~ . - 1, trees)
'trees2 <- eliminate("Girth", trees)'
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ . - 1, trees2)
lapply(list(fit, fit2), coef)
all <- lm(Fertility ~ ., swiss)
summary(all)
summary(lm(Fertility ~ Agriculture, swiss))
cor(swiss$Examination,swiss$Education)
cor(swiss$Agriculture,swiss$Education)
makelms()
ec <- swiss$Examination+swiss$Catholic
efit <- lm(Fertility ~ . + ec, swiss)
all$coefficients - efit$coefficients
6
dim(InsectSprays)
head(InsectSprays,15)
sA
summary(InsectSprays[,2])
sapply(InsectSprays,class)
fit <- lm(count ~ spray, InsectSprays)
summary(fit)$coef
est <- summary(fit)$coef[,1]
mean(sA)
mean(sB)
nfit <- lm(count ~ spray - 1, InsectSprays)
summary(nfit)$coef
spray2 <- relevel(InsectSprays$spray,"C")
fit2 <- lm(count ~ spray2, InsectSprays)
summary(fit2)$coef
mean(sC)
(fit$coef[2]-fit$coef[3])/1.6011
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
# Make a plot of the outcome (CompressiveStrength) versus the index of the samples.
# Color by each of the variables in the data set (you may find the cut2() function in the Hmisc package useful for turning continuous covariates into factors).
# What do you notice in these plots?
training$index <- seq(1, nrow(training))
require(reshape2)
D <- melt(training, id.var=c("index"))
ggplot(D, aes(x=index, y=value, color=variable)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
facet_wrap(~ variable, nrow=3, scales="free_y") +
theme(legend.position="none")
ggplot(training, aes(x=Cement, y=CompressiveStrength)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
geom_rug(alpha=1/4)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
# Make a plot of the outcome (CompressiveStrength) versus the index of the samples.
# Color by each of the variables in the data set (you may find the cut2() function in the Hmisc package useful for turning continuous covariates into factors).
# What do you notice in these plots?
training$index <- seq(1, nrow(training))
require(reshape2)
D <- melt(training, id.var=c("index"))
ggplot(D, aes(x=index, y=value, color=variable)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
facet_wrap(~ variable, nrow=3, scales="free_y") +
theme(legend.position="none")
ggplot(training, aes(x=Cement, y=CompressiveStrength)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
geom_rug(alpha=1/4)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("caret")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$index <- seq(1, nrow(training))
require(reshape2)
D <- melt(training, id.var=c("index"))
ggplot(D, aes(x=index, y=value, color=variable)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
facet_wrap(~ variable, nrow=3, scales="free_y") +
theme(legend.position="none")
ggplot(training, aes(x=Cement, y=CompressiveStrength)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
geom_rug(alpha=1/4)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training, geom="histogram")
table(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
str(training[predVar])
preProcess(training[predVar], method="pca", thresh=0.8)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
M0 <- train(training$diagnosis ~ ., data=training[predVar], method="glm")
hat0 <- predict(M0, testing)
confusionMatrix(testing$diagnosis, hat0)
preProc <- preProcess(training[predVar], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[predVar])
M1 <- train(training$diagnosis ~ ., data=trainPC, method="glm")
testPC <- predict(preProc, testing[predVar])
hat1 <- predict(M1, testPC)
confusionMatrix(testing$diagnosis, hat1)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
predVar <- grep("^IL", names(training))
M0 <- train(training$diagnosis ~ ., data=training[predVar], method="glm")
hat0 <- predict(M0, testing)
confusionMatrix(testing$diagnosis, hat0)
preProc <- preProcess(training[predVar], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[predVar])
M1 <- train(training$diagnosis ~ ., data=trainPC, method="glm")
testPC <- predict(preProc, testing[predVar])
hat1 <- predict(M1, testPC)
confusionMatrix(testing$diagnosis, hat1)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
install.packages("e1071")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
model1
model2
predVar <- grep("^IL", names(training))
M0 <- train(training$diagnosis ~ ., data=training[predVar], method="glm")
hat0 <- predict(M0, testing)
confusionMatrix(testing$diagnosis, hat0)
preProc <- preProcess(training[predVar], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[predVar])
M1 <- train(training$diagnosis ~ ., data=trainPC, method="glm")
testPC <- predict(preProc, testing[predVar])
hat1 <- predict(M1, testPC)
confusionMatrix(testing$diagnosis, hat1)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training, geom="histogram")
table(training$Superplasticizer)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
# Make a plot of the outcome (CompressiveStrength) versus the index of the samples.
# Color by each of the variables in the data set (you may find the cut2() function in the Hmisc package useful for turning continuous covariates into factors).
# What do you notice in these plots?
training$index <- seq(1, nrow(training))
require(reshape2)
D <- melt(training, id.var=c("index"))
ggplot(D, aes(x=index, y=value, color=variable)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
facet_wrap(~ variable, nrow=3, scales="free_y") +
theme(legend.position="none")
ggplot(training, aes(x=Cement, y=CompressiveStrength)) +
geom_point(alpha=1/2) +
geom_smooth(alpha=1/2) +
geom_rug(alpha=1/4)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
M <- train(Class ~ ., data=training, method="rpart")
M
M$finalModel
plot(M$finalModel)
text(M$finalModel)
library(manipulate)
manipulate(plot(1:x), x=smider(1,100))
manipulate(plot(1:x), x=slider(1,100))
install.packages("googleVis")
install.packages(c("BatchJobs", "BBmisc", "BH", "checkmate", "colorspace", "data.table", "DBI", "devtools", "digest", "dplyr", "formatR", "Formula", "ggplot2", "gtools", "highr", "httr", "jsonlite", "KernSmooth", "knitr", "magrittr", "manipulate", "microbenchmark", "mime", "R.methodsS3", "R.oo", "R.utils", "RColorBrewer", "Rcpp", "RCurl", "reshape2", "rmarkdown", "RSQLite", "sendmailR", "swirl", "testthat", "zoo"))
install.packages(c("BatchJobs", "BBmisc", "BH", "checkmate",
ยง"colorspace", "data.table", "DBI", "devtools", "digest", "dplyr", "formatR", "Formula", "ggplot2", "gtools", "highr", "httr", "jsonlite", "KernSmooth", "knitr", "magrittr", "manipulate", "microbenchmark", "mime", "R.methodsS3", "R.oo", "R.utils", "RColorBrewer", "Rcpp", "RCurl", "reshape2", "rmarkdown", "RSQLite", "sendmailR", "swirl", "testthat", "zoo"))
install.packages(c("BatchJobs", "BBmisc", "BH", "checkmate", "colorspace", "data.table", "DBI", "devtools", "digest", "dplyr", "formatR", "Formula", "ggplot2", "gtools", "highr", "httr", "jsonlite", "KernSmooth", "knitr", "magrittr", "manipulate", "microbenchmark", "mime", "R.methodsS3", "R.oo", "R.utils", "RColorBrewer", "Rcpp", "RCurl", "reshape2", "rmarkdown", "RSQLite", "sendmailR", "swirl", "testthat", "zoo"))
install.packages(c("BatchJobs", "BBmisc", "BH", "checkmate",
install.packages("devtools")
install.packages(c("BatchJobs", "BBmisc", "BH", "checkmate", "colorspace", "data.table", "DBI", "digest", "dplyr", "formatR", "Formula", "ggplot2", "gtools", "highr", "jsonlite", "KernSmooth", "knitr", "magrittr", "manipulate", "microbenchmark", "mime", "R.methodsS3", "R.oo", "R.utils", "RColorBrewer", "Rcpp", "RCurl", "reshape2", "rmarkdown", "RSQLite", "sendmailR", "swirl", "testthat", "zoo"))
install_github('slidify','ramnathv')
library(devtools)
install_github('slidify','ramnathv')
install.packages("RCurl")
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
setwd("D:/02.Personal/Education/Coursera/20150410 Developing Data Products/Dataset/Rshiny2")
library(shiny)
runApp('Phenotest')
runApp('Phenotest')
runApp('Phenotest')
runApp('Phenotest')
runApp('Phenotest')
runApp('Phenotest')
library(slidify)
library(devtools)
install_github('slidify')
install_github('slidify','ramathv')
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
setwd("~/GitHub/testDeck")
install.packages('formatR')
---
slidify('index.Rmd')
